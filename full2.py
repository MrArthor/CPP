import requests
import json
import os
import openai
from azure.identity import DefaultAzureCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType
from azure.core.credentials import AzureKeyCredential
import tiktoken
import re



    # Use the current user identity to authenticate with Azure OpenAI, Cognitive Search and Blob Storage (no secrets needed, 
    # just use 'az login' locally, and managed identity when deployed on Azure). If you need to use keys, use separate AzureKeyCredential instances with the 
    # keys for each service
    
    # Prompt Clarification:
    # The prompt is asking you to write a Kusto query using the 'requests' table, which has a specific schema with column names and data types listed.
    # The query should retrieve data from the 'requests' table using the specified schema, without any additional functions.
    # It is recommended to carefully review the schema and column names to ensure the query is constructed correctly.
    # Once completed, the query should be able to retrieve data from the 'requests' table using the specified schema.

main_message = """ Write a Kusto query using the table 'requests' with the following schema:
    'timestamp': datetime
    'id': string
    'source': string
    'name': string
    'url': string
    'success': string
    'resultCode': string
    'duration': real
    'performanceBucket': string
    'itemType': string
    'customDimensions': dynamic
    'customMeasurements': dynamic
    'operation_Name': string
    'operation_Id': string
    'operation_ParentId': string
    'operation_SyntheticSource': string
    'session_Id': string
    'user_Id': string
    'user_AuthenticatedId': string
    'user_AccountId': string
    'application_Version': string
    'client_Type': string
    'client_Model': string
    'client_OS': string
    'client_IP': string
    'client_City': string
    'client_StateOrProvince': string
    'client_CountryOrRegion': string
    'client_Browser': string
    'cloud_RoleName': string
    'cloud_RoleInstance': string
    'appId': string
    'appName': string
    'iKey': string
    'sdkVersion': string
    'itemId': string
    'itemCount': int

    do not use over and sum to generate the query
    """
new_message = f"<|im_start|>system\n{main_message.strip()}\n<|im_end|>"



    # Defining a function to create the prompt from the new message and the messages
    # The function assumes `messages` is a list of dictionaries with `sender` and `text` keys
    # Example: messages = [{"sender": "user", "text": "I want to write a blog post about my company."}]
def app_prompt(new_message, messages):
    prompt = new_message
    for message in messages:
        prompt += f"\n<|im_start|>{message['sender']}\n{ message['text']}\n<|im_end|>"
    prompt += "\n<|im_start|>assistant\n"
    return prompt
    # Defining a function to estimate the number of tokens in a prompt
def estimate_tokens(prompt):
    cl100k_base = tiktoken.get_encoding("cl100k_base") 

    enc = tiktoken.Encoding( 
        name="chatgpt",  
        pat_str=cl100k_base._pat_str, 
        mergeable_ranks=cl100k_base._mergeable_ranks, 
        special_tokens={ 
                **cl100k_base._special_tokens, 
                "<|im_start|>": 100264, 
                "<|im_end|>": 100265
            } 
        ) 
    tokens = enc.encode(prompt,  allowed_special={"<|im_start|>", "<|im_end|>"})
    return len(tokens)
    # Estimate the number of tokens in the system message. Tokens in the system message will be sent in every request.
    # Create the full prompt
    # Defining a function to send the prompt to the ChatGPT model and create kusto query
def send_message(base_prompt, model_name, max_response_tokens=500):
    response = openai.Completion.create(
            engine=model_name,
            prompt=base_prompt,
            temperature=0,
            max_tokens=max_response_tokens,
            stop=['<|im_end|>']
        )
    return response['choices'][0]['text'].strip()
         

    # Set up API credentials and constants
    # Take Kusto query as input from response generated by OpenAi

    # Print query results
        
    # This is the new prompt 



    # Defining a function to create the prompt from the system message and the messages
    # This is the prompt that will again send to OpenAi 
def create_prompt(system_message, messages):
    prompt = system_message
    for message in messages:
        prompt += f"\n<|im_start|>{message['sender']}\n{ message['text']}\n<|im_end|>"
    prompt += "\n<|im_start|>assistant\n"
    return prompt


    # Defining a function to estimate the number of tokens in a prompt
def estimate_tokens(prompt):
    cl100k_base = tiktoken.get_encoding("cl100k_base") 

    enc = tiktoken.Encoding( 
            name="chatgpt",  
            pat_str=cl100k_base._pat_str, 
            mergeable_ranks=cl100k_base._mergeable_ranks, 
            special_tokens={ 
                **cl100k_base._special_tokens, 
                "<|im_start|>": 100264, 
                "<|im_end|>": 100265
            } 
        ) 

    tokens = enc.encode(prompt,  allowed_special={"<|im_start|>", "<|im_end|>"})
    return len(tokens)


    # Estimate the number of tokens in the system message. Tokens in the system message will be sent in every request.

        
    # Create the full prompt
    # print(prompt)   


    # Defining a function to send the prompt to the ChatGPT model
def send_message(prompt, model_name, max_response_tokens=500):
    response = openai.Completion.create(
            engine=model_name,
            prompt=prompt,
            temperature=0,
            max_tokens=max_response_tokens,
            stop=['<|im_end|>']
        )
    return response['choices'][0]['text'].strip()


    # Defining a function to print out the conversation in a readable format
def print_conversation(messages):
    for message in messages:
            print(f"[{message['sender'].upper()}]")
            print(message['text'])
            print()






